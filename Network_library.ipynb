{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "Activation=[]                                   #activations of all the neurons of all layers\n",
    "Derivative_Activation=[]                        #derivatives of activations of all neurons of all layers\n",
    "LR=0.01                                         #learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Network:\n",
    "    \"\"\"\n",
    "    Class represents the neural network wireframe.\n",
    "    \"\"\"\n",
    "    def __init__(self,N_Input_Nodes,N_Hidden_Nodes,N_Output_Nodes):\n",
    "        \"\"\"\n",
    "        constructor method to initialize the neural network parameters.\n",
    "        \"\"\"\n",
    "        print(\"\\n Neural network Initializing........\\n\")\n",
    "        self.N_Input_Nodes=N_Input_Nodes\n",
    "        self.N_Output_Nodes=N_Output_Nodes\n",
    "        self.N_Hidden_Nodes=N_Hidden_Nodes\n",
    "        i=0\n",
    "        self.Hidden_Layers=len(self.N_Hidden_Nodes)\n",
    "        self.Weight_Input_Hidden=Random_Matrix(self.N_Input_Nodes,self.N_Hidden_Nodes[0])\n",
    "        self.Weight_Hidden_Hidden=[]\n",
    "        self.max_iteration=20\n",
    "        while(i<(self.Hidden_Layers-1)):\n",
    "            temp=Random_Matrix(self.N_Hidden_Nodes[i],self.N_Hidden_Nodes[i+1])\n",
    "            self.Weight_Hidden_Hidden.append(temp)\n",
    "            i=i+1 \n",
    "        self.Weight_Hidden_Output=Random_Matrix(self.N_Hidden_Nodes[self.Hidden_Layers-1],self.N_Output_Nodes)\n",
    "        i=0\n",
    "        self.bias_Hidden=[]\n",
    "        while(i<self.Hidden_Layers):\n",
    "            temp=Random_Matrix(self.N_Hidden_Nodes[i],1)\n",
    "            self.bias_Hidden.append(temp)\n",
    "            i=i+1 \n",
    "        self.bias_Output=Random_Matrix(self.N_Output_Nodes,1)\n",
    "#        Show(\"Weight_Input_Hidden\",Weight_Input_Hidden)\n",
    " #       Show(\"Weight_Hidden_Hidden\",Weight_Hidden_Hidden)\n",
    "  #      Show(\"Weight_Hidden_Output\",Weight_Hidden_Output)\n",
    "   #     Show(\"bias_Hidden\",bias_Hidden)\n",
    "    #    Show(\"bias_Output\",bias_Output)\n",
    "        print(\"\\n Neural network Initialized........ \\n\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Activation_Function(z):\n",
    "    \"\"\"\n",
    "    Function returns the sigmoid value matrix of the matrix passed in as paramter.\n",
    "    F([M(m x n)])=[F(M(m x n))]\n",
    "    \"\"\"\n",
    "    #print(\"\\n __________Activating_________ \\n\")\n",
    "    i=0\n",
    "    col=len(z[0])\n",
    "    row=len(z)\n",
    "    activation=Random_Matrix(row,col)\n",
    "    while(i<row):\n",
    "        j=0\n",
    "        while(j<col):\n",
    "            activation[i][j]=(1/(1+math.exp(-0.001*z[i][j])))\n",
    "            j=j+1\n",
    "        i=i+1\n",
    "    #print(\"\\n __________Activated_________ \\n\")\n",
    "    print(activation)\n",
    "    return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Feedforward(Inputs,Weight,Bias,flag):\n",
    "    \"\"\"\n",
    "    Function implements the feed forward procedure of propagating inputs into hidden layers which are activated \n",
    "    and feed to the output layer inturn obtaining the activation/firing status of output neurons.\n",
    "    \"\"\"\n",
    "    #print(\"\\n __________Feeding Forward_________ \\n\")\n",
    "    if(flag==0):\n",
    "        Transposed_Weight=Transpose(Weight)\n",
    "        Weighted_Sum=Matrix_Multiply(Transposed_Weight,Inputs)\n",
    "        Weighted_Sum=Matrix_Sum(Weighted_Sum,Bias)\n",
    "        Activation.append(Activation_Function(Weighted_Sum))\n",
    "        Derivative_Activation.append(Activation_Function_Prime(Weighted_Sum))\n",
    "        #print(\"\\n __________Feedforward Accomplished_________ \\n\")\n",
    "        return(Activation_Function(Weighted_Sum))\n",
    "    else:\n",
    "        for i in range(len(Weight)):\n",
    "            Transposed_Weight=Transpose(Weight[i])\n",
    "            Weighted_Sum=Matrix_Multiply(Transposed_Weight,Inputs)\n",
    "            Weighted_Sum=Matrix_Sum(Weighted_Sum,Bias[i+1])\n",
    "            Activation.append(Activation_Function(Weighted_Sum))\n",
    "            Derivative_Activation.append(Activation_Function_Prime(Weighted_Sum))\n",
    "        #print(\"\\n __________Feedforward Accomplished_________ \\n\")\n",
    "        return(Activation_Function(Weighted_Sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Matrix_Sum(matrix1,matrix2):\n",
    "    \"\"\"\n",
    "    Function returns the summation of two matrices passed as parameters to it.\n",
    "    \"\"\"\n",
    "    row1=len(matrix1)\n",
    "    row2=len(matrix2)\n",
    "    col1=len(matrix1[0])\n",
    "    col2=len(matrix2[0])\n",
    "    i=0\n",
    "    j=0\n",
    "    if((row1==row2) and (col1==col2)):\n",
    "        matrix=Random_Matrix(row1,col1)\n",
    "        while(i<row1):\n",
    "            while(j<col1):\n",
    "                matrix[i][j]=matrix1[i][j]+matrix2[i][j]\n",
    "                j=j+1\n",
    "            i=i+1\n",
    "        return(matrix)\n",
    "    else:\n",
    "        print(\"\\nMatrices incompatible for summation operation!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Matrix_Multiply(matrix1,matrix2):\n",
    "    \"\"\"\n",
    "    Function returns the matrix multiplication of two matrices passed as parameters of it.\n",
    "    \"\"\"\n",
    "    row1=len(matrix1)\n",
    "    row2=len(matrix2)\n",
    "    col1=len(matrix1[0])\n",
    "    col2=len(matrix2[0])\n",
    "    i=0\n",
    "    j=0\n",
    "    k=0\n",
    "    sum=0\n",
    "    matrix=Random_Matrix(row1,col2)\n",
    "    if(col1==row2):\n",
    "        while(i<row1):\n",
    "            while(j<col2):\n",
    "                while(k<col1):\n",
    "                    sum=sum+matrix1[i][k]*matrix2[k][j]\n",
    "                    k=k+1\n",
    "                matrix[i][j]=sum\n",
    "                sum=0\n",
    "                j=j+1\n",
    "            i=i+1\n",
    "        return(matrix)\n",
    "    else:\n",
    "        print(\"\\nMatrices incompatible for matrix multiplication operation!\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Hadamard_Product(matrix1,matrix2):\n",
    "    \"\"\"\n",
    "    Function returns the elementwise product of two matrices.\n",
    "    say, we have two Matrices as [M(m x n)] and [N(m x n)]\n",
    "    then Hadamarad_product(M,N)=[M(i,j)*N(i,j)].\n",
    "    Both [M(m x n)] and [N(m x n)] should be of same shape.\n",
    "    \"\"\"\n",
    "    row1=len(matrix1)\n",
    "    row2=len(matrix2)\n",
    "    col1=len(matrix1[0])\n",
    "    col2=len(matrix2[0])\n",
    "    i=0\n",
    "    matrix=Random_Matrix(row1,col1)\n",
    "    if((col1==col2)and(row1==row2)):\n",
    "        while(i<row1):\n",
    "            j=0\n",
    "            while(j<col1):\n",
    "                matrix[i][j]=matrix1[i][j]*matrix2[i][j]\n",
    "                j=j+1\n",
    "            i=i+1\n",
    "        return(matrix)\n",
    "    else:\n",
    "        print(\"\\nMatrices incompatible for Hadamard matrix multiplication operation!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Random_Matrix(row,col):\n",
    "    \"\"\"\n",
    "    Function returns the matrix with random values, of size specified in the prameter list.\n",
    "    \"\"\"\n",
    "    matrix=[]\n",
    "    i=0\n",
    "    while(i<row):\n",
    "        matrix.append([np.random.uniform(-10,10) for w in range(col)])\n",
    "        i=i+1\n",
    "    return(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Transpose(matrix1):\n",
    "    \"\"\"\n",
    "    Function implements simple matrix transpose operation and returns the resulting matrix.\n",
    "    Transpose([M(n x m)])=[N(m x n)]-----> with rows interchanged with corresponding columns.\n",
    "    \"\"\"\n",
    "    row=len(matrix1)\n",
    "    col=len(matrix1[0])\n",
    "    matrix=Random_Matrix(col,row)\n",
    "    i=0\n",
    "    while(i<row):\n",
    "        j=0\n",
    "        while(j<col):\n",
    "            matrix[j][i]=matrix1[i][j]\n",
    "            j=j+1\n",
    "        i=i+1\n",
    "    return(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Train(brain,Inputs,Label,Weight_Input_Hidden,Weight_Hidden_Hidden,Weight_Hidden_Output,bias_Hidden,bias_Output,Hidden_Layers):\n",
    "    #print(\"\\n __________Training Started_________ \\n\")\n",
    "    \"\"\"\n",
    "    Function to initiate training process for the neural network and check for halting condition\n",
    "    to save the parameters and produce a deployable/testable model.\n",
    "    \"\"\"\n",
    "    Output_Input_Hidden=Feedforward(Inputs,Weight_Input_Hidden,bias_Hidden[0],0)\n",
    "    Output_Hidden_Hidden=Feedforward(Output_Input_Hidden,Weight_Hidden_Hidden,bias_Hidden,1)\n",
    "    Output_Hidden_Output=Feedforward(Output_Hidden_Hidden,Weight_Hidden_Output,bias_Output,0)\n",
    "    if(dist(Output_Hidden_Output[0],Label[0])>0.0016 and brain.max_iteration>0):      #there can be more than one dependent attribute in case of multiclass classification  \n",
    "        brain.max_iteration-=1\n",
    "        Gradient_Descent(brain,Label,Hidden_Layers,Inputs,Weight_Input_Hidden,Weight_Hidden_Hidden,Weight_Hidden_Output,bias_Hidden,bias_Output)\n",
    "    else:\n",
    "        #print(\"\\n __________Training Complete_________ \\n\")\n",
    "        k=open(\"Train.txt\",'w')                                                     #Train.txt contains values of paramers\n",
    "        k.write(\"Weight_Input_Hidden: \"+str(Weight_Input_Hidden)+\"\\n\")              #for future references.\n",
    "        k.write(\"Weight_Hidden_Hidden: \"+str(Weight_Hidden_Hidden)+\"\\n\")\n",
    "        k.write(\"Weight_Hidden_Output: \"+str(Weight_Hidden_Output)+\"\\n\")\n",
    "        k.write(\"bias_Hidden: \"+str(bias_Hidden)+\"\\n\")\n",
    "        k.write(\"bias_Output: \"+str(bias_Output)+\"\\n\")\n",
    "        brain.Weight_Input_Hidden=Weight_Input_Hidden\n",
    "        brain.Weight_Hidden_Hidden=Weight_Hidden_Hidden\n",
    "        brain.Weight_Hidden_Output=Weight_Hidden_Output\n",
    "        brain.bias_Hidden=bias_Hidden\n",
    "        brain.bias_Output=bias_Output\n",
    "        k.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Matrix_Difference(matrix1,matrix2):\n",
    "    \"\"\"\n",
    "    Function is to calculate the difference of two matrices and return the resulting matrix.\n",
    "    \"\"\"\n",
    "    row1=len(matrix1)\n",
    "    row2=len(matrix2)\n",
    "    col1=len(matrix1[0])\n",
    "    col2=len(matrix2[0])\n",
    "    i=0\n",
    "    j=0\n",
    "    if((row1==row2) and (col1==col2)):\n",
    "        matrix=Random_Matrix(row1,col1)\n",
    "        while(i<row1):\n",
    "            while(j<col1):\n",
    "                matrix[i][j]=matrix1[i][j]-matrix2[i][j]\n",
    "                j=j+1\n",
    "            i=i+1\n",
    "        return(matrix)\n",
    "    else:\n",
    "        print(\"\\nMatrices incompatible for Difference operation!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Gradient_Descent(brain,Label,Hidden_Layers,Inputs,Weight_Input_Hidden,Weight_Hidden_Hidden,Weight_Hidden_Output,bias_Hidden,bias_Output):\n",
    "    \"\"\"\n",
    "    Function to Train the network such that total quadratic cost is minimum by changing parameter values in the direction of \n",
    "    negative gradient.\n",
    "    \"\"\"\n",
    "    Layer=Hidden_Layers\n",
    "    error=Matrix_Difference(Activation[Layer],Label)\n",
    "    Backpropagation(brain,error,Layer,Inputs,Weight_Input_Hidden,Weight_Hidden_Hidden,Weight_Hidden_Output,bias_Hidden,bias_Output,Label)\n",
    "    return    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Backpropagation(brain,error,Layer,Inputs,Weight_Input_Hidden,Weight_Hidden_Hidden,Weight_Hidden_Output,bias_Hidden,bias_Output,Label):\n",
    "    \"\"\"\n",
    "    Function to propagate error occuring at output layer to the hidden nodes and updating the parameters in interim.\n",
    "    \"\"\"\n",
    "    Hidden_Layers=Layer\n",
    "    delta_weight_hidden_output=Matrix_Multiply(error,Derivative_Activation[Layer])\n",
    "    delta_bias_output=delta_weight_hidden_output\n",
    "    temp=Transpose(Activation[Layer-1])\n",
    "    delta_weight_hidden_output=Matrix_Multiply(delta_weight_hidden_output,temp)\n",
    "    for i in range(len(delta_weight_hidden_output)):\n",
    "        for j in range(len(delta_weight_hidden_output[0])):\n",
    "            delta_weight_hidden_output[i][j]=delta_weight_hidden_output[i][j]*2*LR\n",
    "    for i in range(len(delta_bias_output)):\n",
    "        for j in range(len(delta_bias_output[0])):\n",
    "            delta_bias_output[i][j]=delta_bias_output[i][j]*2*LR\n",
    "    Layer-=1\n",
    "    error=Matrix_Multiply(Weight_Hidden_Output,error)\n",
    "    Weight_Hidden_Output=Matrix_Difference(Weight_Hidden_Output,Transpose(delta_weight_hidden_output))\n",
    "    bias_Output=Matrix_Difference(bias_Output,delta_bias_output)\n",
    "    while(Layer>0):\n",
    "        delta_weight=Hadamard_Product(error,Derivative_Activation[Layer])    #checkpoint......\n",
    "        delta_bias=delta_weight\n",
    "        delta_weight=Matrix_Multiply(Activation[Layer-1],Transpose(delta_weight))\n",
    "        for i in range(len(delta_weight)):\n",
    "            for j in range(len(delta_weight[0])):\n",
    "                delta_weight[i][j]=delta_weight[i][j]*2*LR\n",
    "        for i in range(len(delta_bias)):\n",
    "            for j in range(len(delta_bias[0])):\n",
    "                delta_bias[i][j]=delta_bias[i][j]*2*LR\n",
    "        error=Matrix_Multiply(Weight_Hidden_Hidden[Layer-1],error)\n",
    "        bias_Hidden[Layer]=Matrix_Difference(bias_Hidden[Layer],delta_bias)\n",
    "        Weight_Hidden_Hidden[Layer-1]=Matrix_Difference(Weight_Hidden_Hidden[Layer-1],delta_weight)\n",
    "        Layer-=1\n",
    "    delta_weight=Hadamard_Product(error,Derivative_Activation[Layer])\n",
    "    delta_bias=delta_weight\n",
    "    delta_weight=Matrix_Multiply(Inputs,Transpose(delta_weight))\n",
    "    for i in range(len(delta_weight)):\n",
    "        for j in range(len(delta_weight[0])):\n",
    "            delta_weight[i][j]=delta_weight[i][j]*2*LR\n",
    "    for i in range(len(delta_bias)):\n",
    "        for j in range(len(delta_bias[0])):\n",
    "            delta_bias[i][j]=delta_bias[i][j]*2*LR\n",
    "    bias_Hidden[Layer]=Matrix_Difference(bias_Hidden[Layer],delta_bias)\n",
    "    Weight_Input_Hidden=Matrix_Difference(Weight_Input_Hidden,delta_weight)\n",
    "    Train(brain,Inputs,Label,Weight_Input_Hidden,Weight_Hidden_Hidden,Weight_Hidden_Output,bias_Hidden,bias_Output,Hidden_Layers)\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Show(item_name,item):\n",
    "    \"\"\"\n",
    "    Helper Function to resolve and debug the code, it prints out the parameter name and its value \n",
    "    in an understandable format.\n",
    "    \"\"\"\n",
    "    print(\"\\n\\n\")\n",
    "    print(item_name)\n",
    "    print(item)\n",
    "    print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Activation_Function_Prime(z):\n",
    "    \"\"\"\n",
    "    Function to calculate the derivative of the activation function on Z i.e. F'(Z)\n",
    "    and return it to the caller function.\n",
    "    \"\"\"\n",
    "    F_X=Activation_Function(z)\n",
    "    row=len(z)\n",
    "    col=len(z[0])\n",
    "    i=0\n",
    "    unitary_negative=Random_Matrix(row,col)\n",
    "    while(i<row):\n",
    "        j=0\n",
    "        while(j<col):\n",
    "            unitary_negative[i][j]=-1\n",
    "            j=j+1\n",
    "        i=i+1\n",
    "    F_Minus_X=Hadamard_Product(F_X,unitary_negative)\n",
    "    derivative=Hadamard_Product(F_X,F_Minus_X)\n",
    "    return(derivative)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dist(a,b):\n",
    "    \"\"\"\n",
    "    Function returns the Euclidean distance between two vectors.\n",
    "    \"\"\"\n",
    "    if(len(a)==len(b)):\n",
    "        dist_sum=0\n",
    "        for i in range(len(a)):\n",
    "            dist_sum+=(a[i]-b[i])**2\n",
    "        return(dist_sum)\n",
    "    else:\n",
    "        return(99999999)                        #if shape of two vectors are not same then they are infinitely distinct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Train_Network(Inputs,Label,N_Input_Nodes,N_Hidden_Nodes,N_Output_Nodes):#Train function to define object of Network class\n",
    "                                                                            #and carry out trainning. \n",
    "    \"\"\"\n",
    "    n ------->number of dependent atrributes. \n",
    "    m ------->number of Labels we are using for trainig of the network.\n",
    "    input format is: \n",
    "    [[[X1],[X2],[X3],......,[Xn]],[[X1],[X2],[X3],......,[Xn]],[[X1],[X2],[X3],......,[Xn]],....(m times)..,[[X1],[X2],[X3],......,[Xn]]]\n",
    "    where X1,X2,X3,....,Xn are neumerical entities(attributes for classification).\n",
    "    \n",
    "    Label format is:\n",
    "    [[L1],[L2],[L3],.....,[Lm]]\n",
    "    where L1,L2,L3,......,Lm are binary entities(label associated with each [[X1],[X2],[X3],......,[Xn]] attribute pair)\n",
    "    \"\"\"\n",
    "    \n",
    "    for i in range(len(Inputs)):\n",
    "        print(\"...........Data vector[{}] training started.......\".format(i+1))\n",
    "        inputs=Inputs[i]\n",
    "        label=[]\n",
    "        label.append(Label[i])\n",
    "        if(i==0):\n",
    "            brain=Network(N_Input_Nodes,N_Hidden_Nodes,N_Output_Nodes)\n",
    "        #epoch=0                                         #controlling the loops of recursive training to a certain number\n",
    "        Train(brain,inputs,label,brain.Weight_Input_Hidden,brain.Weight_Hidden_Hidden,brain.Weight_Hidden_Output,brain.bias_Hidden,brain.bias_Output,brain.Hidden_Layers)\n",
    "        print(\"...........Data vector[{}] training completed.......\".format(i+1))\n",
    "    parameter={\"N_Input_Nodes\":N_Input_Nodes,\"N_Hidden_Nodes\":N_Hidden_Nodes,\"N_Output_Nodes\":N_Output_Nodes,\"Weight_Input_Hidden\":brain.Weight_Input_Hidden,\"Weight_Hidden_Hidden\":brain.Weight_Hidden_Hidden,\"Weight_Hidden_Output\":brain.Weight_Hidden_Output,\"bias_Hidden\":brain.bias_Hidden,\"bias_Output\":brain.bias_Output,\"Hidden_Layers\":brain.Hidden_Layers}\n",
    "    return(parameter)        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Test_Network(Inputs,Label,Parameter):\n",
    "    \"\"\"\n",
    "    n ------->number of dependent atrributes. \n",
    "    m ------->number of Labels we are using for testing of the network.\n",
    "    input format is: \n",
    "    [[[X1],[X2],[X3],......,[Xn]],[[X1],[X2],[X3],......,[Xn]],[[X1],[X2],[X3],......,[Xn]],....(m times)..,[[X1],[X2],[X3],......,[Xn]]]\n",
    "    where X1,X2,X3,....,Xn are neumerical entities(attributes for classification).\n",
    "    \n",
    "    Label format is:\n",
    "    [[L1],[L2],[L3],.....,[Lm]]\n",
    "    where L1,L2,L3,......,Lm are binary entities(label associated with each [[X1],[X2],[X3],......,[Xn]] attribute pair)\n",
    "    \"\"\"\n",
    "    Cost=0\n",
    "    for i in range(len(Inputs)):\n",
    "        print(\"...........Data vector[{}] testing started.......\".format(i+1))\n",
    "        inputs=Inputs[i]\n",
    "        label=[]\n",
    "        label.append(Label[i])\n",
    "        if(i==0):\n",
    "            brain=Network(Parameter[\"N_Input_Nodes\"],Parameter[\"N_Hidden_Nodes\"],Parameter[\"N_Output_Nodes\"])\n",
    "            brain.Weight_Input_Hidden=Parameter[\"Weight_Input_Hidden\"]\n",
    "            brain.Weight_Hidden_Hidden=Parameter[\"Weight_Hidden_Hidden\"]\n",
    "            brain.Weight_Hidden_Output=Parameter[\"Weight_Hidden_Output\"]\n",
    "            brain.bias_Hidden=Parameter[\"bias_Hidden\"]\n",
    "            brain.bias_Output=Parameter[\"bias_Output\"]\n",
    "        Input_to_Hidden=Feedforward(inputs,Parameter[\"Weight_Input_Hidden\"],Parameter[\"bias_Hidden\"][0],0)\n",
    "        Hidden_to_Hidden=Feedforward(Input_to_Hidden,Parameter[\"Weight_Hidden_Hidden\"],Parameter[\"bias_Hidden\"],1)\n",
    "        Hidden_to_Output=Feedforward(Hidden_to_Hidden,Parameter[\"Weight_Hidden_Output\"],Parameter[\"bias_Output\"],0)\n",
    "        Cost+=dist(Hidden_to_Output[0],label[0])\n",
    "        print(\"...........Data vector[{}] testing completed.......\".format(i+1))\n",
    "    return(Cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Model(Inputs,Parameter):\n",
    "    \"\"\"\n",
    "    n ------->number of dependent atrributes. \n",
    "    input format is: \n",
    "    [[[X1],[X2],[X3],......,[Xn]],[[X1],[X2],[X3],......,[Xn]],[[X1],[X2],[X3],......,[Xn]],....(m times)..,[[X1],[X2],[X3],......,[Xn]]]\n",
    "    where X1,X2,X3,....,Xn are neumerical entities(attributes for classification).\n",
    "    It returns the vector of predicted outputs for unlabelled inputs. \n",
    "    \"\"\"\n",
    "    Output=[]\n",
    "    for i in range(len(Inputs)):\n",
    "        print(\"...........Data vector[{}] feed started.......\".format(i+1))\n",
    "        inputs=Inputs[i]\n",
    "        if(i==0):\n",
    "            brain=Network(Parameter[\"N_Input_Nodes\"],Parameter[\"N_Hidden_Nodes\"],Parameter[\"N_Output_Nodes\"])\n",
    "            brain.Weight_Input_Hidden=Parameter[\"Weight_Input_Hidden\"]\n",
    "            brain.Weight_Hidden_Hidden=Parameter[\"Weight_Hidden_Hidden\"]\n",
    "            brain.Weight_Hidden_Output=Parameter[\"Weight_Hidden_Output\"]\n",
    "            brain.bias_Hidden=Parameter[\"bias_Hidden\"]\n",
    "            brain.bias_Output=Parameter[\"bias_Output\"]\n",
    "        Input_to_Hidden=Feedforward(inputs,Parameter[\"Weight_Input_Hidden\"],Parameter[\"bias_Hidden\"][0],0)\n",
    "        Hidden_to_Hidden=Feedforward(Input_to_Hidden,Parameter[\"Weight_Hidden_Hidden\"],Parameter[\"bias_Hidden\"],1)\n",
    "        Hidden_to_Output=Feedforward(Hidden_to_Hidden,Parameter[\"Weight_Hidden_Output\"],Parameter[\"bias_Output\"],0)\n",
    "        Show(\"Prediction: \",Hidden_to_Output[0])\n",
    "        Output.append(Hidden_to_Output[0])\n",
    "        print(\"...........Data vector[{}] feed completed.......\".format(i+1))\n",
    "    return(Output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k=Train_Network([[[1],[2],[3],[4]],[[6],[7],[4],[8]]],[[1],[0]],4,[3,2],1)    #sample call for Training method \n",
    "#c=Test_Network([[[1],[2],[3],[4]],[[6],[7],[4],[8]]],[[1],[0]],k)             #sample call for testing method\n",
    "#Out=Model([[[5],[6],[7],[8]]],k)                                              #sample call for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
